{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd93459-ad03-41b4-b079-6314862ad584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Purpose Operations with Dataset.\n",
    "import pandas as pd\n",
    "\n",
    "# Sentiment Analysis.\n",
    "import nltk\n",
    "\n",
    "# Tokenization.\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Bag-Of-Words.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# TF-IDF.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Stemming.\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Lemmatization.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c57314-4a8e-49e5-91a7-90e83e09983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list:\n",
    "    '''\n",
    "    Tokenizes to words and while tokenizing, the algorithm removes the special characters.\n",
    "\n",
    "    Parameters:\n",
    "    text: str\n",
    "        List of characters to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    list\n",
    "        Tokens have no special characters.\n",
    "    '''\n",
    "\n",
    "    # Remove Special Characters.\n",
    "    text = ''.join(t for t in text if t.isalnum() or t == ' ')\n",
    "\n",
    "    # Tokenizing.\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(words: list) -> list:\n",
    "    '''\n",
    "    Removing the stopwords from the given set of words.\n",
    "\n",
    "    Parameters:\n",
    "    words: list\n",
    "        List of words to remove the stopwords from.\n",
    "\n",
    "    Returns:\n",
    "    list\n",
    "        List of words that do not contain any stopwords.\n",
    "    '''\n",
    "\n",
    "    return [word.lower() for word in words if word.lower() not in set(stopwords.words('English'))]\n",
    "\n",
    "def extract_bag_of_words(words: list) -> tuple:\n",
    "    '''\n",
    "    Extracting Bag-Of-Words from the given `text`.\n",
    "\n",
    "    Parameters:\n",
    "    words: list\n",
    "        List of `words` to extract the bag of words from.\n",
    "\n",
    "    Returns:\n",
    "    tuple\n",
    "        1. Feature Names.\n",
    "        2. Bag of Words.\n",
    "    '''\n",
    "\n",
    "    # Creating an instance of Count-Vectorizer.\n",
    "    count = CountVectorizer()\n",
    "\n",
    "    # Finding bag of words.\n",
    "    bag_of_word = count.fit_transform(words)\n",
    "\n",
    "    # Providing feature names along with the bag of words.\n",
    "    return count.get_feature_names_out(), bag_of_word.toarray()\n",
    "\n",
    "def find_tf_idf(words: str) -> tuple:\n",
    "    '''\n",
    "    Finds the TF-IDF values based on the given words.\n",
    "\n",
    "    Parameters:\n",
    "    words: list\n",
    "        List of `words` to find the TF-IDF from.\n",
    "\n",
    "    Returns:\n",
    "    tuple\n",
    "        1. Feature Matrix.\n",
    "        2. TF-IDF Vocabulary.\n",
    "    '''\n",
    "\n",
    "    # Creating an instance of TF-IDF Vectorizer.\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    # Finding the TF-IDF Vectors.\n",
    "    feature_matrix = tfidf.fit_transform(words)\n",
    "\n",
    "    # Providing feature matrix and its frequency.\n",
    "    return feature_matrix, tfidf.vocabulary_\n",
    "\n",
    "def text_stemming(text: str) -> str:\n",
    "    '''\n",
    "    Stems the given `text`.\n",
    "\n",
    "    Parameters:\n",
    "    text: list\n",
    "        List of characters to find the stemming text.\n",
    "\n",
    "    Returns:\n",
    "    str\n",
    "        Filters the stemming text.\n",
    "    '''\n",
    "\n",
    "    return PorterStemmer().stem(text)\n",
    "\n",
    "def text_lemmatization(text: str) -> str:\n",
    "    '''\n",
    "    Lemmatizes the given `text`.\n",
    "\n",
    "    Parameters:\n",
    "    text: list\n",
    "        List of characters to find the stemming text.\n",
    "\n",
    "    Returns:\n",
    "    str\n",
    "        Filters the lemmatized words.\n",
    "    '''\n",
    "\n",
    "    return WordNetLemmatizer().lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "065ed828-12b1-4dc6-83d5-9a3be66c54de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). these new reduced set of features should then be able to summarize most of the information contained in the original set of features!!!. in this way, a summarised version of the original features can be created from a combination of the original set!!! \n",
      "\n",
      "Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features!!!. In this way, a summarised version of the original features can be created from a combination of the original set!!! \n",
      "\n",
      "(array(['able', 'aims', 'combination', 'contained', 'created', 'creating',\n",
      "       'dataset', 'discarding', 'existing', 'extraction', 'feature',\n",
      "       'features', 'information', 'new', 'number', 'ones', 'original',\n",
      "       'reduce', 'reduced', 'set', 'summarised', 'summarize', 'version',\n",
      "       'way'], dtype=object), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0]], dtype=int64)) \n",
      "\n",
      "(<35x24 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 35 stored elements in Compressed Sparse Row format>, {'feature': 10, 'extraction': 9, 'aims': 1, 'reduce': 17, 'number': 14, 'features': 11, 'dataset': 6, 'creating': 5, 'new': 13, 'existing': 8, 'ones': 15, 'discarding': 7, 'original': 16, 'reduced': 18, 'set': 19, 'able': 0, 'summarize': 21, 'information': 12, 'contained': 3, 'way': 23, 'summarised': 20, 'version': 22, 'created': 4, 'combination': 2}) \n",
      "\n",
      "another commonly used technique to reduce the number of features in a dataset is feature selection! the difference between feature selection and/or feature extraction is that feature selection aims instead to $ rank the importance of the existing features in the dataset and discard less important ones (no new features are created)?!. if you are interested in finding out more about feature selection, you can find more information about it in my previous article. \n",
      "\n",
      "Another commonly used technique to reduce the number of features in a dataset is Feature Selection! The difference between Feature Selection and/or Feature Extraction is that feature selection aims instead to $ rank the importance of the existing features in the dataset and discard less important ones (no new features are created)?!. If you are interested in finding out more about Feature Selection, you can find more information about it in my previous article. \n",
      "\n",
      "(array(['aims', 'andor', 'another', 'article', 'commonly', 'created',\n",
      "       'dataset', 'difference', 'discard', 'existing', 'extraction',\n",
      "       'feature', 'features', 'find', 'finding', 'importance',\n",
      "       'important', 'information', 'instead', 'interested', 'less', 'new',\n",
      "       'number', 'ones', 'previous', 'rank', 'reduce', 'selection',\n",
      "       'technique', 'used'], dtype=object), array([[0, 0, 1, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 1],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)) \n",
      "\n",
      "(<40x30 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in Compressed Sparse Row format>, {'another': 2, 'commonly': 4, 'used': 29, 'technique': 28, 'reduce': 26, 'number': 22, 'features': 12, 'dataset': 6, 'feature': 11, 'selection': 27, 'difference': 7, 'andor': 1, 'extraction': 10, 'aims': 0, 'instead': 18, 'rank': 25, 'importance': 15, 'existing': 9, 'discard': 8, 'less': 20, 'important': 16, 'ones': 23, 'new': 21, 'created': 5, 'interested': 19, 'finding': 14, 'find': 13, 'information': 17, 'previous': 24, 'article': 3}) \n",
      "\n",
      "in this article, i will walk you through how to apply feature extraction techniques using the kaggle mushroom classification dataset as an example??? our objective will be to try to predict if a mushroom is poisonous or not by looking at the given features. all the code used in this post (and more!) is available on kaggle and on my github account. \n",
      "\n",
      "In this article, I will walk you through how to apply Feature Extraction techniques using the Kaggle Mushroom Classification Dataset as an example??? Our objective will be to try to predict if a Mushroom is poisonous or not by looking at the given features. All the code used in this post (and more!) is available on Kaggle and on my GitHub Account. \n",
      "\n",
      "(array(['account', 'apply', 'article', 'available', 'classification',\n",
      "       'code', 'dataset', 'example', 'extraction', 'feature', 'features',\n",
      "       'github', 'given', 'kaggle', 'looking', 'mushroom', 'objective',\n",
      "       'poisonous', 'post', 'predict', 'techniques', 'try', 'used',\n",
      "       'using', 'walk'], dtype=object), array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]], dtype=int64)) \n",
      "\n",
      "(<27x25 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 27 stored elements in Compressed Sparse Row format>, {'article': 2, 'walk': 24, 'apply': 1, 'feature': 9, 'extraction': 8, 'techniques': 20, 'using': 23, 'kaggle': 13, 'mushroom': 15, 'classification': 4, 'dataset': 6, 'example': 7, 'objective': 16, 'try': 21, 'predict': 19, 'poisonous': 17, 'looking': 14, 'given': 12, 'features': 10, 'code': 5, 'used': 22, 'post': 18, 'available': 3, 'github': 11, 'account': 0}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unit Testing.\n",
    "data = ['Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features!!!. In this way, a summarised version of the original features can be created from a combination of the original set!!!',\n",
    "        'Another commonly used technique to reduce the number of features in a dataset is Feature Selection! The difference between Feature Selection and/or Feature Extraction is that feature selection aims instead to $ rank the importance of the existing features in the dataset and discard less important ones (no new features are created)?!. If you are interested in finding out more about Feature Selection, you can find more information about it in my previous article.',\n",
    "        'In this article, I will walk you through how to apply Feature Extraction techniques using the Kaggle Mushroom Classification Dataset as an example??? Our objective will be to try to predict if a Mushroom is poisonous or not by looking at the given features. All the code used in this post (and more!) is available on Kaggle and on my GitHub Account.']\n",
    "\n",
    "for d in data:\n",
    "    # Tokenization.\n",
    "    tokens = remove_stopwords(words = tokenize(text = d))\n",
    "\n",
    "    # Stemming.\n",
    "    print(text_stemming(text = d), '\\n')\n",
    "\n",
    "    # Lemmatization.\n",
    "    print(text_lemmatization(text = d), '\\n')\n",
    "\n",
    "    # Bag of Words.\n",
    "    print(extract_bag_of_words(words = tokens), '\\n')\n",
    "\n",
    "    # TF-IDF.\n",
    "    print(find_tf_idf(words = tokens), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
